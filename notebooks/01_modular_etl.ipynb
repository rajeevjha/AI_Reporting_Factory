{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c090061e",
   "metadata": {},
   "source": [
    "# 01_modular_etl\n",
    "Modular ETL pipeline (Ingest -> Enrich -> Validate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45111688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# CONFIG - update for your environment\n",
    "catalog = \"finance\"\n",
    "schema = \"kyc_ml\"\n",
    "raw_table = f\"{catalog}.{schema}.raw_paysim\"\n",
    "enriched_table = f\"{catalog}.{schema}.customer_enriched\"\n",
    "validated_table = f\"{catalog}.{schema}.validated_data\"\n",
    "source_path = \"dbfs:/FileStore/paysim.csv\"   # change to your path\n",
    "\n",
    "def ingest(path=source_path):\n",
    "    df = (spark.read\n",
    "          .option(\"header\",\"true\")\n",
    "          .option(\"inferSchema\",\"true\")\n",
    "          .csv(path))\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(raw_table)\n",
    "    print(\"Ingested ->\", raw_table)\n",
    "    return df\n",
    "\n",
    "def enrich():\n",
    "    df = spark.table(raw_table)\n",
    "    df = df.withColumn(\"step\", F.col(\"step\").cast(\"int\"))\n",
    "    df = df.withColumn(\"accountType\", F.col(\"type\"))\n",
    "    df = df.withColumn(\"date\", F.date_add(F.to_date(F.lit(\"2020-01-01\")), (F.col(\"step\")/24).cast(\"int\")))\n",
    "    for col in (\"nameOrig\",\"nameDest\"):\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(enriched_table)\n",
    "    print(\"Enriched ->\", enriched_table)\n",
    "    return df\n",
    "\n",
    "def validate():\n",
    "    df = spark.table(enriched_table)\n",
    "    df_valid = df.filter((F.col(\"amount\").isNotNull()) & (F.col(\"amount\") > 0))\n",
    "    df_valid.write.format(\"delta\").mode(\"overwrite\").saveAsTable(validated_table)\n",
    "    print(\"Validated ->\", validated_table)\n",
    "    return df_valid\n",
    "\n",
    "# Run pipeline:\n",
    "# ingest()\n",
    "# enrich()\n",
    "# validate()\n",
    "print('Notebook loaded. Call ingest(), enrich(), validate() to run pipeline.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
